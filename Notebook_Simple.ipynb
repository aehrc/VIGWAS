{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Notebook\n",
    "### This notebook is an exampl that shows how to use [VariantSpark](https://github.com/aehrc/VariantSpark) with [Hail v0.2 library](https://hail.is/)\n",
    "### For demonstration this notebook uses samle dataset available in [ViGWAS](https://github.com/aehrc/VIGWAS)\n",
    "### You must copy ViGWAS reposetory to your S3 and set the path in below cell.\n",
    "### >>> If you use our VariantSpark AWS marketplace VIGWAS is already copeid to your S3. You need to replace your bucket name only.\n",
    "\n",
    "## >>>>>>> [Your feedback is valuable to us please click here for 1 minutes survey](https://docs.google.com/forms/d/e/1FAIpQLScWoazw3-rgNFrZ5vcHL9JUmO0AX6Ji2P54Z2jNJZ-RAObuPg/viewform?usp=sf_link) <<<<<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_Path='s3://YOUR_BUCKET_OR_PATH/ViGWAS/'\n",
    "\n",
    "## Some configs\n",
    "numCPU = 32\n",
    "memory = '60g'\n",
    "numPartitions = numCPU*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment init\n",
    "\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "import hail as hl\n",
    "import varspark.hail as vshl\n",
    "vshl.init(sc=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, FactorRange, LabelSet, Label\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import d3\n",
    "from bokeh.core.properties import value\n",
    "from bokeh.embed import file_html\n",
    "from bokeh.resources import CDN\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models.mappers import CategoricalColorMapper\n",
    "\n",
    "from pprint import pprint\n",
    "output_notebook()\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import math as math\n",
    "import sys\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "import subprocess\n",
    "from itertools import cycle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.import_vcf(path=S3_Path+'sample_input/V1.vcf.bgz',\n",
    "                   skip_invalid_loci=True,\n",
    "                   min_partitions=int(numPartitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Annotation Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annot = hl.import_table(S3_Path+'sample_input/hipster.csv',\n",
    "                        impute=True, delimiter=',').key_by('Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate dataset with sample annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_cols(pheno = Annot[mt.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, pcs, loadings = hl.hwe_normalized_pca(mt.GT, k=2)\n",
    "mt = mt.annotate_cols(pcs = pcs[mt.s].scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.scatter(pcs.scores[0], pcs.scores[1],\n",
    "                    label=mt.cols()[pcs.s].pheno.Hipster,\n",
    "                    title='PCA Case/Control', xlabel='PC1', ylabel='PC2', collect_all=True)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = [mt.pheno.isFemale, mt.pcs[0], mt.pcs[1]]\n",
    "\n",
    "result = hl.logistic_regression_rows(test ='wald', \n",
    "                                          y=mt.pheno.isCase,\n",
    "                                          x=mt.GT.n_alt_alleles(),\n",
    "                                          covariates=covariates)\n",
    "\n",
    "mt = mt.annotate_rows( logreg = result[mt.locus, mt.alleles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.manhattan(result.p_value)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant-Spark RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = vshl.random_forest_model(y=mt.pheno.isCase, x=mt.GT.n_alt_alleles(),\n",
    "                                    seed = 13, mtry_fraction = 0.1,\n",
    "                                    min_node_size = 10, max_depth = 15)\n",
    "\n",
    "rf_model.fit_trees(n_trees=100, batch_size=25)\n",
    "\n",
    "print(\"OOB: \", rf_model.oob_error())\n",
    "impTable = rf_model.variable_importance()\n",
    "\n",
    "mt = mt.annotate_rows(vs_score = impTable[mt.locus, mt.alleles].importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_rows(vs_stats = mt.aggregate_rows(hl.agg.stats(mt['vs_score'])))\n",
    "mt = mt.annotate_rows(z_score = (mt['vs_score'] - mt.vs_stats.mean)/mt.vs_stats.stdev)\n",
    "mt = mt.annotate_rows(vs_score_converted = 10** -mt.z_score)\n",
    "title = 'Variant-Spark Manhattan plot'\n",
    "id_link = 'man-vs'\n",
    "folder = 'Variant Spark'\n",
    "hover_fields = {'rsid': mt.rsid, 'vs_score': mt.vs_score}\n",
    "p = hl.plot.manhattan(pvals=mt.vs_score_converted, hover_fields=hover_fields, title=title)\n",
    "p.yaxis.axis_label = 'Z score of importantce score by VS'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe matrix Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write MT to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.write(S3_Path+'my_dataset.mt',overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
